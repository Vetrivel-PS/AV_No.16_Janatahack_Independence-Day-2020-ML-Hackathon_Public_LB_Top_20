# -*- coding: utf-8 -*-
"""Simple_Transformers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jyt9EpDGQ9xd_vQlAohusIlIoWj60v-E
"""

!pip install --upgrade transformers
!pip install simpletransformers
# memory footprint support libraries/code
!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi
!pip install gputil
!pip install psutil
!pip install humanize
!pip install pyspellchecker

import psutil
import humanize
import os
import GPUtil as GPU

GPUs = GPU.getGPUs()
gpu = GPUs[0]
def printm():
    process = psutil.Process(os.getpid())
    print("Gen RAM Free: " + humanize.naturalsize(psutil.virtual_memory().available), " |     Proc size: " + humanize.naturalsize(process.memory_info().rss))
    print("GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))
printm()

import numpy as np
import pandas as pd
from google.colab import files
from tqdm import tqdm
import warnings
warnings.simplefilter('ignore')
import gc
from scipy.special import softmax
from simpletransformers.classification import ClassificationModel
from sklearn.model_selection import train_test_split, StratifiedKFold, KFold
import sklearn
from sklearn.metrics import log_loss
from sklearn.metrics import *
from sklearn.model_selection import *
from ast import literal_eval
import re
from spellchecker import SpellChecker
import random
from sklearn.preprocessing import LabelEncoder
import torch
pd.options.display.max_colwidth = 200

def seed_all(seed_value):
    random.seed(seed_value) # Python
    np.random.seed(seed_value) # cpu vars
    torch.manual_seed(seed_value) # cpu  vars
    
    if torch.cuda.is_available(): 
        torch.cuda.manual_seed(seed_value)
        torch.cuda.manual_seed_all(seed_value) # gpu vars
        torch.backends.cudnn.deterministic = True  #needed
        torch.backends.cudnn.benchmark = False

seed_all(2)

# spell = SpellChecker()
# def get_correct_words(x):
#     new_sentence = []
#     for i in x.split():
#         check_mispelled = spell.unknown([i])
#         if len(check_mispelled) > 0:
#             for word in check_mispelled:
#                 word_correct = spell.correction(word)
#                 new_sentence.append(word_correct)
#         else:
#             new_sentence.append(i)
#     return ' '.join(new_sentence)

# from albumentations.core.transforms_interface import DualTransform, BasicTransform
# from nltk import sent_tokenize
# class NLPTransform(BasicTransform):
#     """ Transform for nlp task."""
#     LANGS = {
#         'en': 'english'
#     }

#     @property
#     def targets(self):
#         return {"data": self.apply}
    
#     def update_params(self, params, **kwargs):
#         if hasattr(self, "interpolation"):
#             params["interpolation"] = self.interpolation
#         if hasattr(self, "fill_value"):
#             params["fill_value"] = self.fill_value
#         return params

#     def get_sentences(self, text, lang='en'):
#         return sent_tokenize(text, self.LANGS.get(lang, 'english'))

# class SwapWordsTransform(NLPTransform):
#     """ Swap words next to each other """
#     def __init__(self, swap_distance=1, swap_probability=0.1, always_apply=False, p=0.5):
#         """  
#         swap_distance - distance for swapping words
#         swap_probability - probability of swapping for one word
#         """
#         super(SwapWordsTransform, self).__init__(always_apply, p)
#         self.swap_distance = swap_distance
#         self.swap_probability = swap_probability
#         self.swap_range_list = list(range(1, swap_distance+1))

#     def apply(self, data, **params):
#         text, lang = data
#         words = text.split()
#         words_count = len(words)
#         if words_count <= 1:
#             return text, lang

#         new_words = {}
#         for i in range(words_count):
#             if random.random() > self.swap_probability:
#                 new_words[i] = words[i]
#                 continue
    
#             if i < self.swap_distance:
#                 new_words[i] = words[i]
#                 continue
    
#             swap_idx = i - random.choice(self.swap_range_list)
#             new_words[i] = new_words[swap_idx]
#             new_words[swap_idx] = words[i]

#         return ' '.join([v for k, v in sorted(new_words.items(), key=lambda x: x[0])]), lang

# class CutOutWordsTransform(NLPTransform):
#     """ Remove random words """
#     def __init__(self, cutout_probability=0.05, always_apply=False, p=0.5):
#         super(CutOutWordsTransform, self).__init__(always_apply, p)
#         self.cutout_probability = cutout_probability

#     def apply(self, data, **params):
#         text, lang = data
#         words = text.split()
#         words_count = len(words)
#         if words_count <= 1:
#             return text, lang
        
#         new_words = []
#         for i in range(words_count):
#             if random.random() < self.cutout_probability:
#                 continue
#             new_words.append(words[i])

#         if len(new_words) == 0:
#             return words[random.randint(0, words_count-1)], lang

#         return ' '.join(new_words), lang

from google.colab import drive
drive.mount('/content/drive/')

train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/train.csv')
test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/test.csv')
sample_sub = pd.read_csv('/content/drive/My Drive/Colab Notebooks/sample_submission.csv')
train.ABSTRACT = train.ABSTRACT.str.replace("\n"," ")
test.ABSTRACT = test.ABSTRACT.str.replace("\n"," ")

train["text"] = train["TITLE"] + train["ABSTRACT"]
test["text"] = test["TITLE"] + test["ABSTRACT"]

target_labels = ["Computer Science" ,"Physics" , "Mathematics" ,"Statistics" , "Quantitative Biology" , "Quantitative Finance"]
train['label'] = train[target_labels].values.tolist()

le = LabelEncoder()
train['label'] = le.fit_transform(train['label'].astype(str))
train = train[["text","label"]]
test = test[["text"]]
test["label"] = 0

# CONTRACTION_MAP = {
# "ain't": "is not",
# "aren't": "are not",
# "can't": "cannot",
# "can't've": "cannot have",
# "'cause": "because",
# "could've": "could have",
# "couldn't": "could not",
# "couldn't've": "could not have",
# "didn't": "did not",
# "doesn't": "does not",
# "don't": "do not",
# "hadn't": "had not",
# "hadn't've": "had not have",
# "hasn't": "has not",
# "haven't": "have not",
# "he'd": "he would",
# "he'd've": "he would have",
# "he'll": "he will",
# "he'll've": "he he will have",
# "he's": "he is",
# "how'd": "how did",
# "how'd'y": "how do you",
# "how'll": "how will",
# "how's": "how is",
# "I'd": "I would",
# "I'd've": "I would have",
# "I'll": "I will",
# "I'll've": "I will have",
# "I'm": "I am",
# "I've": "I have",
# "i'd": "i would",
# "i'd've": "i would have",
# "i'll": "i will",
# "i'll've": "i will have",
# "i'm": "i am",
# "i've": "i have",
# "isn't": "is not",
# "it'd": "it would",
# "it'd've": "it would have",
# "it'll": "it will",
# "it'll've": "it will have",
# "it's": "it is",
# "let's": "let us",
# "ma'am": "madam",
# "mayn't": "may not",
# "might've": "might have",
# "mightn't": "might not",
# "mightn't've": "might not have",
# "must've": "must have",
# "mustn't": "must not",
# "mustn't've": "must not have",
# "needn't": "need not",
# "needn't've": "need not have",
# "o'clock": "of the clock",
# "oughtn't": "ought not",
# "oughtn't've": "ought not have",
# "shan't": "shall not",
# "sha'n't": "shall not",
# "shan't've": "shall not have",
# "she'd": "she would",
# "she'd've": "she would have",
# "she'll": "she will",
# "she'll've": "she will have",
# "she's": "she is",
# "should've": "should have",
# "shouldn't": "should not",
# "shouldn't've": "should not have",
# "so've": "so have",
# "so's": "so as",
# "that'd": "that would",
# "that'd've": "that would have",
# "that's": "that is",
# "there'd": "there would",
# "there'd've": "there would have",
# "there's": "there is",
# "they'd": "they would",
# "they'd've": "they would have",
# "they'll": "they will",
# "they'll've": "they will have",
# "they're": "they are",
# "they've": "they have",
# "to've": "to have",
# "wasn't": "was not",
# "we'd": "we would",
# "we'd've": "we would have",
# "we'll": "we will",
# "we'll've": "we will have",
# "we're": "we are",
# "we've": "we have",
# "weren't": "were not",
# "what'll": "what will",
# "what'll've": "what will have",
# "what're": "what are",
# "what's": "what is",
# "what've": "what have",
# "when's": "when is",
# "when've": "when have",
# "where'd": "where did",
# "where's": "where is",
# "where've": "where have",
# "who'll": "who will",
# "who'll've": "who will have",
# "who's": "who is",
# "who've": "who have",
# "why's": "why is",
# "why've": "why have",
# "will've": "will have",
# "won't": "will not",
# "won't've": "will not have",
# "would've": "would have",
# "wouldn't": "would not",
# "wouldn't've": "would not have",
# "y'all": "you all",
# "y'all'd": "you all would",
# "y'all'd've": "you all would have",
# "y'all're": "you all are",
# "y'all've": "you all have",
# "you'd": "you would",
# "you'd've": "you would have",
# "you'll": "you will",
# "you'll've": "you will have",
# "you're": "you are",
# "you've": "you have"
# }

# #from contractions import CONTRACTION_MAP
# import re
# import nltk
# import string
# from nltk.stem import WordNetLemmatizer
# from html.parser import HTMLParser
# import unicodedata
# import nltk
# nltk.download('stopwords')
# nltk.download('punkt')
# stopword_list = nltk.corpus.stopwords.words('english')
# wnl = WordNetLemmatizer
# html_parser = HTMLParser()

# def tokenize_text(text):
#     tokens = nltk.word_tokenize(text) 
#     tokens = [token.strip() for token in tokens]
#     return tokens

# def expand_contractions(text, contraction_mapping):
    
#     contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), 
#                                       flags=re.IGNORECASE|re.DOTALL)
#     def expand_match(contraction):
#         match = contraction.group(0)
#         first_char = match[0]
#         expanded_contraction = contraction_mapping.get(match)\
#                                 if contraction_mapping.get(match)\
#                                 else contraction_mapping.get(match.lower())                       
#         expanded_contraction = first_char+expanded_contraction[1:]
#         return expanded_contraction
        
#     expanded_text = contractions_pattern.sub(expand_match, text)
#     expanded_text = re.sub("'", "", expanded_text)
#     return expanded_text
    
    
# #from pattern.en import tag
# from nltk.corpus import wordnet as wn


# def pos_tag_text(text):
#     def penn_to_wn_tags(pos_tag):
#         if pos_tag.startswith('J'):
#             return wn.ADJ
#         elif pos_tag.startswith('V'):
#             return wn.VERB
#         elif pos_tag.startswith('N'):
#             return wn.NOUN
#         elif pos_tag.startswith('R'):
#             return wn.ADV
#         else:
#             return None
#     tagged_text = tag(text)
#     tagged_lower = [(word.lower(), penn_to_wn_tags(pos_tag)) for word, pos_tag in tagged_text]
#     return tagged_lower


# def lemmatize_text(text):
#     pos_tagged_text = pos_tag_text(text)
#     lemmatized_tokens = [wnl.lemmatize("",word,pos_tag) if pos_tag else word for word, pos_tag in pos_tagged_text]
#     lemmatized_text = ' '.join(lemmatized_tokens)
#     return lemmatized_text    

# def remove_special_characters(text):
#     tokens = tokenize_text(text)
#     pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))
#     filtered_tokens = filter(None, [pattern.sub(' ', token) for token in tokens])
#     filtered_text = ' '.join(filtered_tokens)
#     return filtered_text
    
    
# def remove_stopwords(text):
#     tokens = tokenize_text(text)
#     filtered_tokens = [token for token in tokens if token not in stopword_list]
#     filtered_text = ' '.join(filtered_tokens)    
#     return filtered_text

# def unescape_html(parser, text):
    
#     return parser.unescape(text)

# def normalize_corpus(corpus, lemmatize=True, tokenize=False):
    
#     normalized_corpus = []  
#     for text in corpus:
#         text = html_parser.unescape(text)
#         text = expand_contractions(text, CONTRACTION_MAP)
#         if lemmatize:
#             text = lemmatize_text(text)
#         else:
#             text = text.lower()
#         text = remove_special_characters(text)
#         text = remove_stopwords(text)
#         if tokenize:
#             text = tokenize_text(text)
#             normalized_corpus.append(text)
#         else:
#             normalized_corpus.append(text)
            
#     return normalized_corpus


# def parse_document(document):
#     document = re.sub('\n', ' ', document)
#     if isinstance(document, str):
#         document = document
#     elif isinstance(document, unicode):
#         return unicodedata.normalize('NFKD', document).encode('ascii', 'ignore')
#     else:
#         raise ValueError('Document is not string or unicode!')
#     document = document.strip()
#     sentences = nltk.sent_tokenize(document)
#     sentences = [sentence.strip() for sentence in sentences]
#     return sentences

# train['text'] = normalize_corpus(train['text'],lemmatize=False,tokenize=False)
# test['text'] = normalize_corpus(test['text'],lemmatize=False,tokenize=False)

import spacy, re
#Data Cleanup

def pre_process(df):
  df['text']=df['text'].str.replace('\n','')
  df['text']=df['text'].str.replace('\r','')
  df['text']=df['text'].str.replace('\t','')
    
  #This removes unwanted texts
  df['text'] = df['text'].apply(lambda x: re.sub(r'[0-9]','',x))
  df['text'] = df['text'].apply(lambda x: re.sub(r'[/(){}\[\]\|@,;.:-]',' ',x))
    
  #Converting all upper case to lower case
  df['text']= df['text'].apply(lambda s:s.lower() if type(s) == str else s)
    

  #Remove un necessary white space
  df['text']=df['text'].str.replace('  ',' ')

  #Remove Stop words
  nlp=spacy.load("en_core_web_sm")
  df['text'] =df['text'].apply(lambda x: ' '.join([word for word in x.split() if nlp.vocab[word].is_stop==False ]))
  return df

train = pre_process(train)
test = pre_process(test)

# train['text']= train['text'].apply(lambda x: re.sub(r"[^A-Za-z]", " ", x))
# test['text'] = test['text'].apply(lambda x: re.sub(r"[^A-Za-z]", " ", x))
# #--------------------------------------------------------
# train['text'] = train['text'].apply(lambda x:get_correct_words(x))
# test['text'] = test['text'].apply(lambda x:get_correct_words(x))

# train.to_csv("/content/drive/My Drive/Colab Notebooks/JanathaHack/Janatahack: Independence Day 2020 ML Hackathon/regex_train.csv",index=False)
# test.to_csv("/content/drive/My Drive/Colab Notebooks/JanathaHack/Janatahack: Independence Day 2020 ML Hackathon/regex_test.csv",index=False)

# transform = SwapWordsTransform(p=1.0, swap_distance=1, swap_probability=0.2)
# tr=CutOutWordsTransform(p=1.0, cutout_probability=0.2)

# lang = 'en'

# for i in range(len(train)):
#         text = train['text'][i]
#         text=transform(data=(text, lang))['data'][0]
#         train['text'][i]=tr(data=(text, lang))['data'][0]

# print(train['text'].apply(lambda x: len(x.split())).describe())

train

wordcount = train['text'].str.count(' ') + 1
wordcount.mean()

test

train['label'].nunique()

model = ClassificationModel('roberta', 'roberta-base', use_cuda=True,num_labels=24, args={
                                                                         'train_batch_size':32,
                                                                         'reprocess_input_data': True,
                                                                         'overwrite_output_dir': True,
                                                                         'fp16': False,
                                                                         'do_lower_case': False,
                                                                         'num_train_epochs': 3,
                                                                         'max_seq_length': 300,
                                                                         'regression': False,
                                                                         'manual_seed': 2,
                                                                         "learning_rate":5e-5,
                                                                         'weight_decay':0.0,
                                                                         "save_eval_checkpoints": False,
                                                                         "save_model_every_epoch": False,
                                                                         "silent": False}
                                                                         )

model.train_model(train)

tst_result, tst_model_outputs, tst_wrong_predictions = model.eval_model(test)

preds = softmax(tst_model_outputs,axis=1)

# import pickle
# with open("/content/drive/My Drive/Colab Notebooks/JanathaHack/Janatahack: Independence Day 2020 ML Hackathon/preds3.pkl", 'wb') as f:
#     pickle.dump(preds, f)



# # with open("/content/drive/My Drive/Colab Notebooks/JanathaHack/Janatahack: Independence Day 2020 ML Hackathon/preds1.pkl", 'rb') as f:
# #     preds1 = pickle.load(f)
# # with open("/content/drive/My Drive/Colab Notebooks/JanathaHack/Janatahack: Independence Day 2020 ML Hackathon/preds2.pkl", 'rb') as f:
# #     preds2 = pickle.load(f)

# preds_fin = preds1*0.8 + preds2*0.2

# finalpreds = [np.argmax(x) for x in preds_fin]
finalpreds = [np.argmax(x) for x in preds]

sample_sub["target"] = finalpreds
sample_sub["target"] = le.inverse_transform(sample_sub["target"])
sample_sub.loc[:,'target'] = sample_sub.loc[:,'target'].apply(lambda x: literal_eval(x))
sample_sub[target_labels] = pd.DataFrame(sample_sub.target.tolist(), index= sample_sub.index)
sample_sub.drop("target",axis=1,inplace = True)

sub_file_name = "S17_normalize_corpus_Colab_Roberta_BASE_max_seq_300_epochs_3.csv"
sample_sub.to_csv(sub_file_name,index = False)

from google.colab import files
files.download(sub_file_name)

# Ensemble : "Blending of Reboerta-base + Bert-base-uncased" :

sub_file_name = "FINAL_AV_SUBMISSION_Ensemble_bert-uncased_roberta-base_0.80_0.80.csv"
preds_final = preds_2*0.80 + preds_3*0.80

finalpreds_3 = [np.argmax(x) for x in preds_final]

sample_sub["target"] = finalpreds_3
sample_sub["target"] = le.inverse_transform(sample_sub["target"])
sample_sub.loc[:,'target'] = sample_sub.loc[:,'target'].apply(lambda x: literal_eval(x))
sample_sub[target_labels] = pd.DataFrame(sample_sub.target.tolist(), index= sample_sub.index)
sample_sub.drop("target",axis=1,inplace = True)
sample_sub.to_csv(sub_file_name,index = False)

# BEST SCORE of 83.97 - Micro F1 Score


